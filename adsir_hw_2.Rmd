---
title: "adsir_hw_2"
author: "Jake Greenberg"
date: "3/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r warning = FALSE, message = FALSE}
library(tidyverse) # for graphing and data cleaning
library(tidymodels) # for modeling
library(stacks) # for stacking models
library(naniar) # for examining missing values (NAs)
library(lubridate) # for data manipulation
library(moderndive) # for King Country housing data
library(vip) # for variable importance plots
library(DALEX) # for model interpretation
library(forcats)
library(DALEXtra) # for extension of DALEX
library(patchwork) # for combining plots nicely
library(kknn)
library(ranger)
library(glmnet)
library(rpart)

theme_set(theme_minimal()) # Lisa's favorite theme
```

# Would you actually have this variable to predict with at the time you are going to make the prediction
# Looking at removing variables with only one value (zero variance) (step_nzf should get rid of these observations)

Link to GitHub Repository: https://github.com/jgreenb4/adsir_hw_2

# Modeling

```{r}
data("lending_club")
# Data dictionary (as close as I could find): https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691
create_more_bad <- lending_club %>% 
  filter(Class == "bad") %>% 
  sample_n(size = 3000, replace = TRUE)

lending_club_mod <- lending_club %>% 
  bind_rows(create_more_bad)
```

```{r}
lending_club_mod %>% 
  group_by(term) %>% 
  summarize(n())
```


**1. Explore the data, concentrating on examining distributions of variables and examining missing values.**

**2. Do any data cleaning steps that need to happen before the model is build. For example, you might remove any variables that mean the same thing as the response variable (not sure if that happens here), get rid of rows where all variables have missing values, etc.**

```{r}
lending_club_mod %>% 
  ggplot(aes(x = funded_amnt)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = int_rate)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = annual_inc)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = total_bal_il)) + geom_density()

lending_club_mod %>% 
  group_by(Class) %>% 
  summarize(n())

lending_club_mod %>% 
  group_by(term) %>% 
  summarize(n())

lending_club_mod %>% 
  group_by(addr_state) %>% 
  summarize(n())


lending_club_mod %>% 
  group_by(sub_grade) %>% 
  summarize(n())

summary(lending_club_mod)
```

```{r}
sum(is.na(lending_club_mod)) # counts the number of NA values for all variables
```

```{r}
# checks for duplicate observations
lending_club_mod %>% 
  distinct()
```

**3. Split the data into training and test, putting 75% in the training data.**

```{r}
set.seed(494) # for reproducibility
lending_club_split <- initial_split(lending_club_mod, prop = 0.75, strata = "Class")
lending_club_training <- training(lending_club_split)
lending_club_testing <- testing(lending_club_split)
```

**4. Set up the recipe and the pre-processing steps to build a lasso model. Some steps you should take:**

Make all integer variables numeric (I'd highly recommend using `step_mutate_at()` or this will be a lot of code). We'll want to do this for the model interpretation we'll do later. Think about grouping factor variables with many levels. Make categorical variables dummy variables (make sure NOT to do this to the outcome variable). Normalize quantitative variables.  

```{r}
lending_club_recipe <- recipe(Class ~ ., data = lending_club_training) %>% 
  step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>%
  step_rm(acc_now_delinq, delinq_amnt) %>% 
  step_normalize(all_predictors(),
                 -all_nominal()) %>% 
  step_dummy(all_nominal(),
             -all_outcomes())
lending_club_recipe %>% 
  prep(lending_club_training) %>% 
  juice()
```


**5. Set up the lasso model and workflow. We will tune the `penalty` parameter.**

```{r}
lending_club_lasso <- logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("classification")
lending_club_lasso
lending_club_lasso_wf <- 
  workflow() %>% 
  add_recipe(lending_club_recipe) %>% 
  add_model(lending_club_lasso)
lending_club_lasso_wf
```


**6. Set up the model tuning for the `penalty` parameter. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. Find the accuracy and area under the roc curve for the model with the best tuning parameter.  Use 5-fold cv.**

```{r}
set.seed(494)
lending_club_cv <- vfold_cv(lending_club_training, v = 5)
penalty_grid <- grid_regular(penalty(),
                             levels = 20)
control_grid <- control_stack_grid()
lending_club_lasso_tune <- lending_club_lasso_wf %>% 
  tune_grid(resamples = lending_club_cv,
            grid = penalty_grid,
            control= control_grid)
lending_club_lasso_tune
```


```{r}
lending_club_lasso_tune %>%
  show_best(metric = "accuracy")
```


```{r}
lending_club_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.config == "Preprocessor1_Model01")
```


**7. Set up the recipe and the pre-processing steps to build a random forest model. You shouldn't have to do as many steps. The only step you should need to do is making all integers numeric.**

```{r}
ranger_recipe <- recipe(Class ~., data = lending_club_training) %>% 
  step_mutate_at(all_numeric(), fn = ~as.numeric(.))
ranger_recipe %>% 
  prep(lending_club_training) %>% 
  juice()
```


**8. Set up the random forest model and workflow. We will tune the `mtry` and `min_n` parameters and set the number of trees, `trees`, to 100 (otherwise the next steps take too long).**

```{r}
lending_club_ranger <- rand_forest(mtry = tune(), 
              min_n = tune(), 
              trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")
lending_club_ranger_wf <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(lending_club_ranger) 
lending_club_ranger_wf
```


**9. Set up the model tuning for both the `mtry` and `min_n` parameters. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. Use only 3 levels in the grid. For the `mtry` parameter, you need to put `finalize(mtry(), lending_club_training %>% select(-Class))` in as an argument instead of just `mtry()`, where `lending_club_training` is the name of your training data. This is because the `mtry()` grid will otherwise have unknowns in it. This part can take a while to run.**


```{r}
set.seed(494)
rf_grid <- grid_regular(min_n(), finalize(mtry(), lending_club_training %>% select(-Class)), levels = 3)
ctrl_res <- control_stack_grid()
ranger_cv <- lending_club_ranger_wf %>% 
  tune_grid(resamples = lending_club_cv,
           grid = rf_grid,
           control = ctrl_res)
collect_metrics(ranger_cv)
```

**10. Find the best tuning parameters. What is the are the accuracy and area under the ROC curve for the model with those tuning parameters?**

```{r}
ranger_cv %>% 
  show_best(metric = "accuracy")
```

```{r}
ranger_cv %>% 
  collect_metrics() %>% 
  filter(.config == "Preprocessor1_Model4")
```

```{r}
best <- ranger_cv %>% 
  select_best(metric = "accuracy")
lending_club_ranger_final_wf<- lending_club_ranger_wf %>% 
  finalize_workflow(best)
```



**11. Use functions from the `DALEX` and `DALEXtra` libraries to create a histogram and boxplot of the residuals from the training data. How do they look? Any interesting behavior?**


Overall, both of these plots (histogram and boxplot) look reasonably similar to what I would expect. The histogram of residuals is generally cenetered with a large frequency of residuals around 0.0, but appears to be skewed right in its distribution, which could be an issue in creating a non-biased model. I wonder if this could be the result of possible endogeneity or omitted variable bias. The boxplots of residuals show the variance of the residuals; this plot exhibits that overall, the model has relatively small variance.

```{r}
set.seed(494)
ranger_fit <- lending_club_ranger_final_wf %>% 
  fit(lending_club_training)
rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = lending_club_training %>% select(-Class), 
    y = as.numeric(lending_club_training$Class == "good"),
    label = "rf"
  )
rf_mod_perf <-  model_performance(rf_explain)
hist_plot <- 
  plot(rf_mod_perf, 
       geom = "histogram")
box_plot <-
  plot(rf_mod_perf, 
       geom = "boxplot")
hist_plot + box_plot
```


**12. Use `DALEX` functions to create a variable importance plot from this model. What are the most important variables?**

Based on the variable importance plot located below, the most impotant variables included in this model are interest rate, all_util, and annual income.

```{r}
rf_var_imp <- 
  model_parts(
    rf_explain
    )
plot(rf_var_imp)
```


**13. Write a function called `cp_profile` to make a CP profile. The function will take an explainer, a new observation, and a variable name as its arguments and create a CP profile for a quantitative predictor variable. You will need to use the `predict_profile()` function inside the function you create - put the variable name there so the plotting part is easier. You'll also want to use `aes_string()` rather than `aes()` and quote the variables. Use the `cp_profile()` function to create one CP profile of your choosing. Be sure to choose a variable that is numeric, not integer. There seem to be issues with those that I'm looking into.**

For an extra challenge, write a function that will work for either a quantitative or categorical variable.

If you need help with function writing check out the [Functions](https://r4ds.had.co.nz/functions.html) chapter of R4DS by Wickham and Grolemund.

```{r}
cp_profile <- function(x,y,z){
  predict_profile(explainer = x, new_observation = y, variables = z) %>% 
    rename(yhat = `_yhat_`) %>% # we rename yhat to avoid possible naming errors
    ggplot(aes_string(x = z, y = "yhat")) + 
    geom_point()}
obs <- lending_club_training %>%
  slice(3)
obs
cp_profile(rf_explain, obs, "annual_inc")
```


**14. Use `DALEX` functions to create partial dependence plots (with the CP profiles in gray) for the 3-4 most important variables. If the important variables are categorical, you can instead make a CP profile for 3 observations in the dataset and discuss how you could go about constructing a partial dependence plot for a categorical variable (you don't have to code it, but you can if you want an extra challenge). If it ever gives you an error that says, "Error: Can't convert from `VARIABLE` <double> to `VARIABLE` <integer> due to loss of precision", then remove that variable from the list. I seem to have figured out why it's doing that, but I don't know how to fix it yet.**

```{r}
rf_pdp <- model_profile(explainer = rf_explain,
                        variables = c("annual_inc", "int_rate", "open_il_12m"))
cp_annual_inc <- plot(rf_pdp, variables = "annual_inc", geom = "profiles")

cp_annual_inc

cp_int_rate <- plot(rf_pdp, variables = "int_rate", geom = "profiles")
cp_int_rate

cp_open_il_12m <- plot(rf_pdp, variables = "open_il_12m", geom = "profiles")
cp_open_il_12m 
```

**15. Fit one more model type of your choosing that will feed into the stacking model.**

```{r}
lending_club_knn <-
  nearest_neighbor(
    neighbors = tune("k")
  ) %>%
  set_engine("kknn") %>% 
  set_mode("classification")

lending_club_knn_wf <- 
  workflow() %>% 
  add_model(lending_club_knn) %>%
  add_recipe(lending_club_recipe)

lending_club_knn_tune <- 
  lending_club_knn_wf %>% 
  tune_grid(
    lending_club_cv,
    grid = 4,
    control = control_grid
  )
```


**16. Create a model stack with the candidate models from the previous parts of the exercise and use the `blend_predictions()` function to find the coefficients of the stacked model. Create a plot examining the performance metrics for the different penalty parameters to assure you have captured the best one. If not, adjust the penalty. (HINT: use the `autoplot()` function). Which models are contributing most?**

The models carrying the largest weight in contributing to the blended model are two of the random forest models (.pred_good_ranger_cv_1_4 carries the most significant weight).

```{r}
lending_club_stack <- 
  stacks() %>% 
  add_candidates(ranger_cv) %>% 
  add_candidates(lending_club_lasso_tune) %>% 
  add_candidates(lending_club_knn_tune)
lending_club_blend <- lending_club_stack %>% 
  blend_predictions()
lending_club_blend
autoplot(lending_club_blend)
```


**17. Fit the final stacked model using `fit_members()`. Apply the model to the test data and report the accuracy and area under the curve. Create a graph of the ROC and construct a confusion matrix. Comment on what you see. Save this final model using the `saveRDS()` function - see the [Use the model](https://advanced-ds-in-r.netlify.app/posts/2021-03-16-ml-review/#use-the-model) section of the `tidymodels` intro. We are going to use the model in the next part. You'll want to save it in the folder where you create your shiny app.**


```{r}
lending_club_final_stack <- lending_club_blend %>% 
  fit_members()
lending_club_final_stack
```


```{r}
lending_club_stack_test <- lending_club_testing %>% 
  bind_cols(predict(lending_club_final_stack, new_data = lending_club_testing, type = "prob")) %>% 
  bind_cols(predict(lending_club_final_stack, new_data = lending_club_testing))
lending_club_stack_test %>% 
  accuracy(.pred_class, Class)
lending_club_stack_test %>% 
  roc_auc(Class, .pred_bad)
autoplot(roc_curve(lending_club_stack_test, Class, .pred_bad))
```


```{r}
lending_club_stack_test %>% 
  conf_mat(Class, .pred_class)
```


# Coded Bias
----------------------

I thought that the entirety of the Coded Bias film was extremely well-done and thought-provoking. As discussed at length in the Bias and Fairness lecture from Dr. Rachel Thomas, I thought that the issues of sub-group under-representation in the facial recognition software of many different technological kingpins illustrate a far larger scope on the issues of bias in data science and algorithms. I was particularly enticed by the discussion of China/Hong Kong and how the Chinese government exploits the abundance of individual-level data on the protestors to reinforce its oppressive and censoring authority, but also how the protestors in Hong Kong fought back by spray painting over the cameras that could be used to collect information about the identities of the participating protestors. I also felt indecesive about my stance on the government collecting all of this intel about each member of society throughout my viewing of this film; while much of the predatory comercial information that is collected for advertisement targeting may be invasive and violate privacy to a degree, the thoroughness and frequency of this data collection/monitoring for individuals may boast public safety benefits by signaling when someone could be a high risk to commit a violent crime-- assuming that these companies are eventually able to build non-biased models for evaluating this risk that could ultimately be ethically deployed. I also found the overarching topic of our current reality versus the society we strive to be as an imperative topic to consider when creating an algorithm; just because a model's results may perform well at automating manual processes and results, that does not necessarily mean that it is prioritizing the proper ideals. An example of this is the AI tool that Amazon built to sift through resumes which rejected all resumes from women, a direct refelction of the underepresentation of this demographic in their typical hired candidates.

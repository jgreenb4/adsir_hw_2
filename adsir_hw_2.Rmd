---
title: "adsir_hw_2"
author: "Jake Greenberg"
date: "3/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #warning = FALSE, message = FALSE)
```

```{r warning = FALSE, message = FALSE}
library(tidyverse) # for graphing and data cleaning
library(tidymodels) # for modeling
library(stacks) # for stacking models
library(naniar) # for examining missing values (NAs)
library(lubridate) # for data manipulation
library(moderndive) # for King Country housing data
library(vip) # for variable importance plots
library(DALEX) # for model interpretation
library(DALEXtra) # for extension of DALEX
library(patchwork) # for combining plots nicely

theme_set(theme_minimal()) # Lisa's favorite theme
```

# Would you actually have this variable to predict with at the time you are going to make the prediction
# Looking at removing variables with only one value (zero variance) (step_nzf should get rid of these observations)

```{r}
data("lending_club")
# Data dictionary (as close as I could find): https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691
create_more_bad <- lending_club %>% 
  filter(Class == "bad") %>% 
  sample_n(size = 3000, replace = TRUE)

lending_club_mod <- lending_club %>% 
  bind_rows(create_more_bad)
```

```{r}
lending_club_mod %>% 
  group_by(term) %>% 
  summarize(n())
```

```{r}
lending_club_mod %>% 
  ggplot(aes(x = funded_amnt)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = int_rate)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = annual_inc)) + geom_density()

lending_club_mod %>% 
  ggplot(aes(x = total_bal_il)) + geom_density()

lending_club_mod %>% 
  group_by(class) %>% 
  summarize(n())

lending_club_mod %>% 
  group_by(term) %>% 
  summarize(n())

lending_club_mod %>% 
  group_by(addr_state) %>% 
  summarize(n())


lending_club_mod %>% 
  group_by(sub_grade) %>% 
  summarize(n())

summary(lending_club_mod)
```

```{r}
sum(is.na(lending_club_mod)) # counts the number of NA values for all variables
```

```{r}
# checks for duplicate observations
lending_club_mod %>% 
  distinct()
```

```{r}
set.seed(494)
lending_club_split <- initial_split(lending_club_mod, prop = .75, strata = Class)
lending_club_train <- training(lending_club_split)
lending_club_test <- testing(lending_club_split)
```

```{r}
unique(lending_club_train$verification_status)
```

```{r}
lending_club_recipe <- recipe(Class ~ ., data = lending_training) %>% 
  step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>% 
  step_mutate(verification_status = as.factor(ifelse(verification_status == "Source_Verified", "Verified", verification_status))) %>% 
  step_mutate(annual_inc = case_when(annual_inc <= 9875 ~ 10,
                                     annual_inc > 9875 && annual_inc <= 40125 ~ 12,
                                     annual_inc > 40125 && annual_inc <= 85526 ~ 22,
                                     annual_inc > 85526 && annual_inc <= 163300 ~ 24,
                                     annual_inc >163300 && annual_inc <= 207350 ~ 32,
                                     annual_inc > 207350 && annual_inc <= 518400 ~ 35,
                                     annual_inc > 518400 ~ 37)) %>% 
  step_mutate(annual_inc = as.factor(annual_inc)) %>% 
  step_rm(acc_now_delinq, delinq_amnt) %>% 
  step_normalize(all_predictors(),
                 -all_nominal()) %>% 
  step_dummy(all_nominal(),
             -all_outcomes())

lending_club_recipe %>% 
  prep(lending_club_train) %>% 
  juice()
```


Set up the lasso model and workflow. We will tune the penalty parameter.

```{r}
lending_club_lasso_mod <- 
  # Define a linear regression model
  logistic_reg(mixture = 1) %>% 
  # Set the engine to "lm" (lm() function is used to fit model)
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  # Not necessary here, but good to remember for other models
  set_mode("classification")



lending_club_lasso_wf <- 
  # Set up the workflow
  workflow() %>% 
  # Add the recipe
  add_recipe(lending_club_recipe) %>% 
  # Add the modeling
  add_model(lending_club_lasso_mod)

lending_club_lasso_wf
```


```{r}
set.seed(494) #for reproducible 5-fold
lending_club_cv <- vfold_cv(lending_club_train, v = 5)

penalty_grid <- grid_regular(penalty(),
                             levels = 20)
control_grid <- control_stack_grid()

lending_club_lasso_tune <- lending_club_lasso_wf %>% 
  tune_grid(resamples = lending_club_cv,
            grid = penalty_grid,
            control= control_grid)
lending_club_lasso_tune
```


```{r}
lending_club_lasso_tune %>%
  show_best(metric = "accuracy")
```


```{r}
lending_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.config == "Preprocessor1_Model13")
```


# Random Forest
```{r}
lending_club_ranger_recipe <- recipe(Class ~., data = lending_club_train) %>% 
  step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>% 
  step_mutate(verification_status = as.factor(ifelse(verification_status == "Source_Verified", "Verified", verification_status))) %>% 
  step_rm(acc_now_delinq, delinq_amnt)

lending_club_ranger_recipe %>% 
  prep(lending_club_train) %>% 
  juice()
```


```{r}
lending_club_ranger <- rand_forest(mtry = tune(), 
              min_n = tune(), 
              trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")


lending_club_ranger_wf <- 
  workflow() %>% 
  add_recipe(lending_club_ranger_recipe) %>% 
  add_model(lending_club_ranger) 
lending_club_ranger_wf
```


```{r}
set.seed(494)
rf_grid <- grid_regular(min_n(), finalize(mtry(), lending_training %>% select(-Class)), levels = 3)

ctrl_res <- control_stack_grid()
lending_club_ranger_cv <- lending_club_ranger_wf %>% 
  tune_grid(resamples = lending_club_cv,
           grid = rf_grid,
           control = ctrl_res)

collect_metrics(lending_club_ranger_cv)
```

10.

```{r}
lending_club_ranger_cv %>% 
  show_best(metric = "accuracy")
```

```{r}
lending_club_ranger_cv %>% 
  collect_metrics() %>% 
  filter(.config == "Preprocessor1_Model4")
```


11. Use functions from the `DALEX` and `DALEXtra` libraries to create a histogram and boxplot of the residuals from the training data. How do they look? Any interesting behavior?


```{r}
best <- lending_club_ranger_cv %>% 
  select_best(metric = "accuracy")
lending_club_ranger_final_wf<- lending_club_ranger_wf %>% 
  finalize_workflow(best)
```


11. Use functions from the `DALEX` and `DALEXtra` libraries to create a histogram and boxplot of the residuals from the training data. How do they look? Any interesting behavior?

```{r}
set.seed(494)
lending_club_ranger_fit <- lending_club_ranger_final_wf %>% 
  fit(lending_club_train)

lending_club_rf_explain <- 
  explain_tidymodels(
    model = lending_club_ranger_fit,
    data = lending_club_train %>% select(-Class), 
    y = as.numeric(lending_club_train$Class == "good)",
    label = "rf",
    type = "classification"
  ))

lending_club_rf_mod_perf <-  model_performance(lending_club_rf_explain)
hist_plot <- 
  plot(lending_club_rf_mod_perf, 
       geom = "histogram")
box_plot <-
  plot(lending_club_rf_mod_perf, 
       geom = "boxplot")
hist_plot + box_plot
```




12. Use `DALEX` functions to create a variable importance plot from this model. What are the most important variables? 

```{r}
lending_club_rf_var_imp <- 
  model_parts(
    lending_club_rf_explain
    )
plot(lending_club_rf_var_imp)
```


13. Write a function called `cp_profile` to make a CP profile. The function will take an explainer, a new observation, and a variable name as its arguments and create a CP profile for a quantitative predictor variable. You will need to use the `predict_profile()` function inside the function you create - put the variable name there so the plotting part is easier. You'll also want to use `aes_string()` rather than `aes()` and quote the variables. Use the `cp_profile()` function to create one CP profile of your choosing. Be sure to choose a variable that is numeric, not integer. There seem to be issues with those that I'm looking into.

For an extra challenge, write a function that will work for either a quantitative or categorical variable. 

If you need help with function writing check out the [Functions](https://r4ds.had.co.nz/functions.html) chapter of R4DS by Wickham and Grolemund.


14. Use `DALEX` functions to create partial dependence plots (with the CP profiles in gray) for the 3-4 most important variables. If the important variables are categorical, you can instead make a CP profile for 3 observations in the dataset and discuss how you could go about constructing a partial dependence plot for a categorical variable (you don't have to code it, but you can if you want an extra challenge). If it ever gives you an error that says, "Error: Can't convert from `VARIABLE` <double> to `VARIABLE` <integer> due to loss of precision", then remove that variable from the list. I seem to have figured out why it's doing that, but I don't know how to fix it yet.

15. Fit one more model type of your choosing that will feed into the stacking model. 


# Coded Bias
----------------------

I thought that the entirety of the Coded Bias film was extremely well-done and thought-provoking. As discussed at length in the Bias and Fairness lecture from Dr. Rachel Thomas, I thought that the issues of sub-group under-representation in the facial recognition software of many different technological kingpins illustrate a far larger scope on the issues of bias in data science and algorithms. I was particularly enticed by the discussion of China/Hong Kong and how the Chinese government exploits the abundance of individual-level data on the protestors to reinforce its oppressive and censoring authority, but also how the protestors in Hong Kong fought back by spray painting over the cameras that could be used to collect information about the identities of the participating protestors. I also felt indecesive about my stance on the government collecting all of this intel about each member of society throughout my viewing of this film; while much of the predatory comercial information that is collected for advertisement targeting may be invasive and violate privacy to a degree, the thoroughness and frequency of this data collection/monitoring for individuals may boast public safety benefits by signaling when someone could be a high risk to commit a violent crime-- assuming that these companies are eventually able to build non-biased models for evaluating this risk that could ultimately be ethically deployed. I also found the overarching topic of our current reality versus the society we strive to be as an imperative topic to consider when creating an algorithm; just because a model's results may perform well at automating manual processes and results, that does not necessarily mean that it is prioritizing the proper ideals. An example of this is the AI tool that Amazon built to sift through resumes which rejected all resumes from women, a direct refelction of the underepresentation of this demographic in their typical hired candidates.
